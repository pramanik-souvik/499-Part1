{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load LLM for text generation (example: BanglaBERT)\n",
    "text_generator = pipeline(\"text-generation\", model=\"csebuetnlp/banglabert\")\n",
    "\n",
    "def generate_passage(text):\n",
    "    if len(text) < 100:\n",
    "        return None\n",
    "    try:\n",
    "        generated = text_generator(text, max_new_tokens=150, num_return_sequences=1)\n",
    "        return generated[0]['generated_text'] if generated else text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating passage: {e}\")\n",
    "        return text\n",
    "\n",
    "def generate_title(text):\n",
    "    return text[:50] + \"...\" if len(text) > 50 else text\n",
    "\n",
    "def process_data(data):\n",
    "    processed = []\n",
    "    for entry in data:\n",
    "        content = entry.get(\"content\", \"\").strip()\n",
    "        if len(content) >= 100:\n",
    "            passage = generate_passage(content)\n",
    "            if passage:\n",
    "                title = generate_title(passage)\n",
    "                processed.append({\n",
    "                    \"type\": entry[\"type\"],\n",
    "                    \"content\": content,\n",
    "                    \"title\": title,\n",
    "                    \"passage\": passage\n",
    "                })\n",
    "    return processed\n",
    "\n",
    "def process_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    data = df.to_dict(orient='records')\n",
    "    processed = process_data(data)\n",
    "    processed_df = pd.DataFrame(processed)\n",
    "    processed_df.to_csv(\"processed_output.csv\", index=False)\n",
    "    print(\"CSV Processing Done!\")\n",
    "\n",
    "# Example Usage\n",
    "process_csv(\"cleaned_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_data.csv\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_output.csv\")\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load LLM for text generation (BanglaBERT or alternative Bangla models)\n",
    "text_generator = pipeline(\"text-generation\", model=\"csebuetnlp/banglabert\", device=0)\n",
    "\n",
    "def truncate_text(text, max_tokens=512):\n",
    "    \"\"\"Ensures text is within model token limits.\"\"\"\n",
    "    return text[:max_tokens]\n",
    "\n",
    "def generate_passage_batch(batch):\n",
    "    \"\"\"Batch process passages using the text-generation model.\"\"\"\n",
    "    passages = []\n",
    "    for text in batch['content']:\n",
    "        if len(text) < 100:\n",
    "            passages.append(None)\n",
    "        else:\n",
    "            try:\n",
    "                text = truncate_text(text, 512)  # Truncate long texts\n",
    "                generated = text_generator(text, max_new_tokens=150, num_return_sequences=1)\n",
    "                passage = generated[0]['generated_text'] if generated else text\n",
    "                passages.append(passage)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating passage: {e}\")\n",
    "                passages.append(text)\n",
    "    return {\"passage\": passages}\n",
    "\n",
    "def generate_title_batch(batch):\n",
    "    \"\"\"Generate titles from the first 50 characters.\"\"\"\n",
    "    return {\"title\": [text[:50] + \"...\" if len(text) > 50 else text for text in batch['passage']]}\n",
    "\n",
    "def process_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[df['content'].str.len() >= 100]  # Filter short content\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(generate_passage_batch, batched=True, batch_size=8)\n",
    "    dataset = dataset.map(generate_title_batch, batched=True, batch_size=8)\n",
    "    processed_df = dataset.to_pandas()\n",
    "    processed_df.to_csv(\"processed.csv\", index=False)\n",
    "    print(\"CSV Processing Done!\")\n",
    "\n",
    "# Example Usage\n",
    "process_csv(\"merged_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USERAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_predictions.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV Processing Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load LLM compatible with BanglaBERT (use fill-mask since it's not for text-generation)\n",
    "text_generator = pipeline(\"fill-mask\", model=\"csebuetnlp/banglabert\")\n",
    "\n",
    "def generate_passage(text):\n",
    "    # Modify the sentence to insert a mask for fill-mask pipeline\n",
    "    try:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        if len(words) < 20:\n",
    "            return None\n",
    "        midpoint = len(words) // 2\n",
    "        words[midpoint] = text_generator.tokenizer.mask_token\n",
    "        masked_text = \" \".join(words)\n",
    "        result = text_generator(masked_text)\n",
    "        if result:\n",
    "            # Replace mask with top prediction\n",
    "            filled = masked_text.replace(text_generator.tokenizer.mask_token, result[0][\"token_str\"])\n",
    "            return filled\n",
    "        else:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating passage: {e}\")\n",
    "        return text\n",
    "\n",
    "def generate_title(text):\n",
    "    return text[:50] + \"...\" if len(text) > 50 else text\n",
    "\n",
    "def process_data(data):\n",
    "    processed = []\n",
    "    for entry in data:\n",
    "        content = entry.get(\"content\", \"\").strip()\n",
    "        if len(content) >= 100:\n",
    "            passage = generate_passage(content)\n",
    "            if passage:\n",
    "                title = generate_title(passage)\n",
    "                processed.append({\n",
    "                    \"type\": entry.get(\"type\", \"unknown\"),\n",
    "                    \"content\": content,\n",
    "                    \"title\": title,\n",
    "                    \"passage\": passage\n",
    "                })\n",
    "    return processed\n",
    "\n",
    "def process_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    data = df.to_dict(orient='records')\n",
    "    processed = process_data(data)\n",
    "    processed_df = pd.DataFrame(processed)\n",
    "    processed_df.to_csv(\"processed_output1.csv\", index=False)\n",
    "    print(\"✅ CSV Processing Done!\")\n",
    "\n",
    "# Example Usage\n",
    "process_csv(\"dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
